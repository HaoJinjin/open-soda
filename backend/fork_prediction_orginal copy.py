import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
import os
from pathlib import Path
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
import scipy.stats as stats
import warnings
warnings.filterwarnings('ignore')

# ==================== åŸºç¡€é…ç½® ====================
np.random.seed(42)
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['figure.dpi'] = 100

# ==================== 1. åŠ è½½JSONæ•°æ® ====================
print("ã€1/8ã€‘åŠ è½½æ•°æ®ï¼ˆé€’å½’è¯»å–JSONæ–‡ä»¶å¤¹ï¼‰...")
csv_path = r"C:\Users\22390\Desktop\OpenSODA\backendData\top_300_metrics.csv"

def load_json_from_folder(folder_path):
    all_data = []
    for file_path in Path(folder_path).rglob('*.json'):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            if isinstance(json_data, dict):
                all_data.append(json_data)
            elif isinstance(json_data, list):
                all_data.extend(json_data)
            print(f"  âœ“ åŠ è½½æ–‡ä»¶: {file_path}")
        except Exception as e:
            print(f"  âš ï¸  è¯»å–å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
    if not all_data:
        raise ValueError("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆJSONæ•°æ®")
    df = pd.DataFrame(all_data)
    print(f"\nâœ“ åŠ è½½å®Œæˆï¼š{df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    print(f"  åˆ—ååˆ—è¡¨ï¼š{list(df.columns)}")
    return df

try:
    df = load_json_from_folder(root_folder)
except Exception as e:
    print(e)
    exit()

# ==================== 2. é€‰æ‹©ç›®æ ‡åˆ—å¹¶å¤„ç†ç±»å‹ ====================
print("\nã€2/8ã€‘é€‰æ‹©å¹¶å¤„ç†ç›®æ ‡åˆ—...")
# å±•ç¤ºåˆ—åä¾›é€‰æ‹©
cols = list(df.columns)
for idx, col in enumerate(cols):
    print(f"  [{idx}] {col}")

# é€‰æ‹©ç›®æ ‡åˆ—
target_column = None
while not target_column:
    user_input = input("\nè¾“å…¥ç›®æ ‡åˆ—åºå·/åˆ—åï¼š").strip()
    if user_input.isdigit():
        idx = int(user_input)
        if 0 <= idx < len(cols):
            target_column = cols[idx]
        else:
            print(f"âŒ åºå·è¶…å‡ºèŒƒå›´ï¼ˆ0-{len(cols)-1}ï¼‰")
    elif user_input in cols:
        target_column = user_input
    else:
        print(f"âŒ åˆ—å '{user_input}' ä¸å­˜åœ¨")

print(f"âœ“ é€‰ä¸­ç›®æ ‡åˆ—ï¼š{target_column}")

# å¤„ç†ç›®æ ‡åˆ—ç±»å‹
def convert_to_numeric(col_data):
    """æ™ºèƒ½è½¬æ¢ä¸ºæ•°å€¼å‹ï¼ˆå…¼å®¹æ—¶é—´/æ•°å­—ï¼‰"""
    # å°è¯•è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆå¦‚æœæ˜¯æ—¶é—´å­—ç¬¦ä¸²ï¼‰
    try:
        # å¸¸è§æ—¶é—´æ ¼å¼åŒ¹é…
        time_formats = ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%Y/%m/%d', '%Y%m%d']
        for fmt in time_formats:
            try:
                return pd.to_datetime(col_data, format=fmt).astype('int64') // 10**9  # è½¬ç§’çº§æ—¶é—´æˆ³
            except:
                continue
        # è‡ªåŠ¨è¯†åˆ«æ—¶é—´æ ¼å¼
        return pd.to_datetime(col_data).astype('int64') // 10**9
    except:
        # è½¬æ¢ä¸ºæ™®é€šæ•°å€¼
        return pd.to_numeric(col_data, errors='coerce')

# è½¬æ¢ç›®æ ‡åˆ—å¹¶æ¸…ç†ç©ºå€¼
df['target_numeric'] = convert_to_numeric(df[target_column])
df_clean = df.dropna(subset=['target_numeric']).reset_index(drop=True)
print(f"âœ“ ç›®æ ‡åˆ—å¤„ç†å®Œæˆï¼š{len(df_clean)} æ¡æœ‰æ•ˆæ•°æ®")
if len(df_clean) == 0:
    print("âŒ ç›®æ ‡åˆ—æ— æœ‰æ•ˆæ•°å€¼æ•°æ®")
    exit()

# ==================== 3. ç”ŸæˆåŸºç¡€ç‰¹å¾ï¼ˆæ— åŸå§‹ç‰¹å¾æ—¶ï¼‰ ====================
print("\nã€3/8ã€‘ç”Ÿæˆç‰¹å¾...")
# è‡ªåŠ¨ç”ŸæˆåŸºç¡€ç‰¹å¾ï¼ˆåŸºäºç°æœ‰åˆ—ï¼‰
feature_cols = []

# 1. å¯¹æ‰€æœ‰éç›®æ ‡åˆ—å°è¯•è½¬ä¸ºæ•°å€¼ç‰¹å¾
for col in df_clean.columns:
    if col not in [target_column, 'target_numeric']:
        try:
            # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
            df_clean[f'feat_{col}'] = pd.to_numeric(df_clean[col], errors='coerce')
            # å°è¯•è½¬æ¢æ—¶é—´ä¸ºæ—¶é—´æˆ³
            if pd.isna(df_clean[f'feat_{col}']).all():
                df_clean[f'feat_{col}'] = convert_to_numeric(df_clean[col])
            # ç§»é™¤å…¨ç©ºç‰¹å¾
            if not pd.isna(df_clean[f'feat_{col}']).all():
                df_clean[f'feat_{col}'] = df_clean[f'feat_{col}'].fillna(0)
                feature_cols.append(f'feat_{col}')
        except:
            continue

# 2. å¦‚æœæ— ä»»ä½•ç‰¹å¾ï¼Œç”Ÿæˆç®€å•åºåˆ—ç‰¹å¾
if not feature_cols:
    print("âš ï¸  æ— æœ‰æ•ˆç‰¹å¾åˆ—ï¼Œç”Ÿæˆåºåˆ—ç‰¹å¾")
    df_clean['feat_index'] = df_clean.index  # è¡Œç´¢å¼•ç‰¹å¾
    df_clean['feat_random'] = np.random.rand(len(df_clean))  # éšæœºç‰¹å¾ï¼ˆå…œåº•ï¼‰
    feature_cols = ['feat_index', 'feat_random']

print(f"âœ“ ç”Ÿæˆç‰¹å¾ï¼š{feature_cols}")

# ==================== 4. æ•°æ®å‡†å¤‡ ====================
print("\nã€4/8ã€‘æ•°æ®æ‹†åˆ†ä¸æ ‡å‡†åŒ–...")
X = df_clean[feature_cols]
y = df_clean['target_numeric']

# æ‹†åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(
    X, y, df_clean.index, test_size=0.3, random_state=42
)

# æ ‡å‡†åŒ–
scaler_X = RobustScaler()
scaler_y = RobustScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)
y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()

print(f"âœ“ æ‹†åˆ†å®Œæˆï¼šè®­ç»ƒé›† {len(X_train)} æ¡ï¼Œæµ‹è¯•é›† {len(X_test)} æ¡")

# ==================== 5. æ¨¡å‹è®­ç»ƒ ====================
print("\nã€5/8ã€‘è®­ç»ƒæ¨¡å‹...")
# è°ƒæ•´æ¨¡å‹å‚æ•°é€‚é…å°ç‰¹å¾é›†
rf_model = RandomForestRegressor(
    n_estimators=50,  # å‡å°‘æ ‘æ•°é‡
    max_depth=5,      # é™ä½å¤æ‚åº¦
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train_scaled, y_train_scaled)

# é¢„æµ‹å¹¶åæ ‡å‡†åŒ–
y_pred_scaled = rf_model.predict(X_test_scaled)
y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
y_true = y_test.values

# ==================== 6. æ¨¡å‹è¯„ä¼° ====================
print("\nã€6/8ã€‘æ¨¡å‹è¯„ä¼°...")
# è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆå…¼å®¹å¤§æ•°ï¼‰
r2 = r2_score(y_true, y_pred)
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
mae = mean_absolute_error(y_true, y_pred)

# æ ¼å¼åŒ–è¾“å‡ºï¼ˆç§‘å­¦è®¡æ•°æ³•é€‚é…å¤§æ•°ï¼‰
print(f"  RÂ² å¾—åˆ†ï¼š{r2:.4f}")
print(f"  RMSEï¼š{rmse:.2e}")  # ç§‘å­¦è®¡æ•°æ³•
print(f"  MAEï¼š{mae:.2e}")

# ==================== 7. ç”Ÿæˆé¢„æµ‹ç»“æœï¼ˆè¿˜åŸåŸå§‹æ ¼å¼ï¼‰ ====================
print("\nã€7/8ã€‘ç”Ÿæˆé¢„æµ‹ç»“æœ...")
# è¿˜åŸç›®æ ‡å€¼ä¸ºåŸå§‹æ ¼å¼
def revert_numeric_to_original(numeric_val, original_data):
    """å°†æ•°å€¼å‹ç»“æœè¿˜åŸä¸ºåŸå§‹æ ¼å¼"""
    # å¦‚æœæ˜¯æ—¶é—´æˆ³ï¼Œè½¬å›æ—¶é—´å­—ç¬¦ä¸²
    try:
        original_sample = original_data.dropna().iloc[0]
        # æ£€æŸ¥åŸå§‹æ•°æ®æ˜¯å¦ä¸ºæ—¶é—´
        pd.to_datetime(original_sample)
        return datetime.fromtimestamp(int(numeric_val)).strftime('%Y-%m-%d %H:%M:%S')
    except:
        # æ™®é€šæ•°å€¼
        return round(float(numeric_val), 2)

# æ„å»ºç»“æœå­—å…¸
results = {
    "metadata": {
        "target_column": target_column,
        "feature_columns": feature_cols,
        "total_samples": len(df),
        "valid_samples": len(df_clean),
        "train_samples": len(X_train),
        "test_samples": len(X_test),
        "metrics": {
            "R2_score": round(r2, 4),
            "RMSE": f"{rmse:.2e}",
            "MAE": f"{mae:.2e}"
        }
    },
    "predictions": []
}

# å¡«å……é¢„æµ‹ç»“æœ
for i, idx in enumerate(test_idx):
    # åŸå§‹é¡¹ç›®åç§°ï¼ˆå°½é‡è·å–ï¼‰
    proj_name = ""
    for name_col in ['name', 'projectname', 'repo', 'repository']:
        if name_col in df_clean.columns and pd.notna(df_clean.loc[idx, name_col]):
            proj_name = str(df_clean.loc[idx, name_col])
            break
    if not proj_name:
        proj_name = f"é¡¹ç›®_{idx}"
    
    # è¿˜åŸçœŸå®å€¼å’Œé¢„æµ‹å€¼
    true_val = revert_numeric_to_original(y_true[i], df[target_column])
    pred_val = revert_numeric_to_original(y_pred[i], df[target_column])
    
    results["predictions"].append({
        "project_name": proj_name,
        "true_value": true_val,
        "predicted_value": pred_val,
        "absolute_error": round(abs(y_true[i] - y_pred[i]), 2),
        "relative_error_percent": round(abs((y_true[i] - y_pred[i])/(y_true[i]+1e-8))*100, 2)
    })

# ==================== 8. ä¿å­˜ç»“æœ ====================
print("\nã€8/8ã€‘ä¿å­˜ç»“æœ...")
# ä¿å­˜é¢„æµ‹ç»“æœ
output_pred = r'C:\Users\22390\Desktop\OpenSODA\backendData\prediction_results.json'
with open(output_pred, 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=4)

# ä¿å­˜ç‰¹å¾é‡è¦æ€§
importance = pd.DataFrame({
    "feature_name": feature_cols,
    "importance": rf_model.feature_importances_
}).sort_values('importance', ascending=False)

importance_dict = {
    "feature_importance": importance.to_dict('records')
}
output_imp = r'C:\Users\22390\Desktop\OpenSODA\backendData\feature_importance.json'
with open(output_imp, 'w', encoding='utf-8') as f:
    json.dump(importance_dict, f, ensure_ascii=False, indent=4)

print(f"âœ“ é¢„æµ‹ç»“æœï¼š{output_pred}")
print(f"âœ“ ç‰¹å¾é‡è¦æ€§ï¼š{output_imp}")
print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")