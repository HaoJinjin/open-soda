import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import json
import os
from pathlib import Path
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
import scipy.stats as stats
import warnings
warnings.filterwarnings('ignore')

# ==================== åŸºç¡€é…ç½® ====================
np.random.seed(42)
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['figure.dpi'] = 100


# ==================== å°è£…çš„é¢„æµ‹å‡½æ•° ====================
def predict_fork_count(csv_path: str = r"C:\Users\22390\Desktop\OpenSODA\backendData\top_300_metrics.csv") -> dict:
    """
    é¢„æµ‹ Fork æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨ technical_fork åˆ—ï¼Œå³ç¬¬32åˆ—ï¼‰

    å‚æ•°:
        csv_path: CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œæœ‰é»˜è®¤å€¼ï¼‰

    è¿”å›:
        åŒ…å«é¢„æµ‹ç»“æœå’Œç‰¹å¾é‡è¦æ€§çš„å­—å…¸
        {
            "metadata": {...},           # å…ƒæ•°æ®ä¿¡æ¯ï¼ˆåŒ…å«æ¨¡å‹é€‰æ‹©ã€æ€§èƒ½æŒ‡æ ‡ç­‰ï¼‰
            "model_comparison": {...},   # æ¨¡å‹å¯¹æ¯”ç»“æœ
            "predictions": [...],        # é¢„æµ‹ç»“æœåˆ—è¡¨
            "feature_importance": [...]  # ç‰¹å¾é‡è¦æ€§åˆ—è¡¨
        }
    """
    # å›ºå®šä½¿ç”¨ç¬¬32åˆ—ï¼štechnical_fork
    target_column = "technical_fork"

    return predict_target_column(csv_path, target_column)


def predict_target_column(csv_path: str, target_column: str) -> dict:
    """
    å¯¹æŒ‡å®šçš„ç›®æ ‡åˆ—è¿›è¡Œé¢„æµ‹ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰- å¤šæ¨¡å‹å¯¹æ¯”ç‰ˆæœ¬

    å‚æ•°:
        csv_path: CSVæ–‡ä»¶è·¯å¾„
        target_column: ç›®æ ‡åˆ—åç§°

    è¿”å›:
        åŒ…å«é¢„æµ‹ç»“æœå’Œç‰¹å¾é‡è¦æ€§çš„å­—å…¸
        {
            "metadata": {...},           # å…ƒæ•°æ®ä¿¡æ¯
            "model_comparison": {...},   # æ¨¡å‹å¯¹æ¯”ç»“æœ
            "predictions": [...],        # é¢„æµ‹ç»“æœ
            "feature_importance": [...]  # ç‰¹å¾é‡è¦æ€§
        }
    """
    try:
        # 1. åŠ è½½æ•°æ®
        df = pd.read_csv(csv_path, encoding='utf-8')

        # 2. å¤„ç†ç›®æ ‡åˆ—
        def convert_to_numeric(col_data):
            """æ™ºèƒ½è½¬æ¢ä¸ºæ•°å€¼å‹"""
            try:
                numeric = pd.to_numeric(col_data, errors='coerce')
                if numeric.isna().mean() < 0.5:
                    return numeric
            except:
                pass

            try:
                time_formats = ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%Y/%m/%d', '%Y%m%d']
                for fmt in time_formats:
                    try:
                        return pd.to_datetime(col_data, format=fmt, errors='raise').astype('int64') // 10**9
                    except:
                        continue
                return pd.to_datetime(col_data, errors='coerce').astype('int64') // 10**9
            except:
                return pd.to_numeric(col_data, errors='coerce')

        df['target_numeric'] = convert_to_numeric(df[target_column])
        df_clean = df.dropna(subset=['target_numeric']).reset_index(drop=True)

        if len(df_clean) == 0:
            raise ValueError("ç›®æ ‡åˆ—æ— æœ‰æ•ˆæ•°å€¼æ•°æ®")

        # 3. ç‰¹å¾å·¥ç¨‹ï¼ˆæ™ºèƒ½é€‰æ‹©ç‰¹å¾ï¼‰
        potential_features = []

        # å€™é€‰ç‰¹å¾åˆ—è¡¨
        candidate_features = [
            'bus_factor', 'change_requests', 'change_requests_accepted',
            'change_requests_reviews', 'code_change_lines_add',
            'code_change_lines_remove', 'inactive_contributors',
            'issues_closed', 'issues_new', 'issue_comments',
            'new_contributors'
        ]

        # è½¬æ¢ä¸ºæ•°å€¼ç‰¹å¾å¹¶è®¡ç®—ç›¸å…³æ€§
        for col in candidate_features:
            if col in df_clean.columns:
                try:
                    df_clean[f'feat_{col}'] = pd.to_numeric(df_clean[col], errors='coerce')
                    corr = df_clean[f'feat_{col}'].corr(df_clean['target_numeric'])
                    if abs(corr) < 0.95:  # æ’é™¤è¿‡é«˜ç›¸å…³æ€§ï¼ˆé¿å…æ•°æ®æ³„æ¼ï¼‰
                        median_val = df_clean[f'feat_{col}'].median()
                        df_clean[f'feat_{col}'] = df_clean[f'feat_{col}'].fillna(median_val)
                        potential_features.append(f'feat_{col}')
                except:
                    continue

        # å¦‚æœç‰¹å¾å¤ªå°‘ï¼Œæ·»åŠ è¡ç”Ÿç‰¹å¾
        if len(potential_features) < 3:
            if 'feat_code_change_lines_add' in df_clean.columns and 'feat_code_change_lines_remove' in df_clean.columns:
                df_clean['feat_code_change_ratio'] = (df_clean['feat_code_change_lines_add'] + 1) / (df_clean['feat_code_change_lines_remove'] + 1)
                potential_features.append('feat_code_change_ratio')

            if 'feat_issues_closed' in df_clean.columns and 'feat_issues_new' in df_clean.columns:
                df_clean['feat_issue_resolution_rate'] = (df_clean['feat_issues_closed'] + 1) / (df_clean['feat_issues_new'] + 1)
                potential_features.append('feat_issue_resolution_rate')

        feature_cols = potential_features if potential_features else ['feat_index']

        # å¦‚æœæ²¡æœ‰ä»»ä½•ç‰¹å¾ï¼Œä½¿ç”¨ç´¢å¼•
        if not potential_features:
            df_clean['feat_index'] = df_clean.index
            feature_cols = ['feat_index']

        # 4. æ•°æ®å‡†å¤‡
        X = df_clean[feature_cols]
        y = df_clean['target_numeric']

        X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(
            X, y, df_clean.index, test_size=0.3, random_state=42
        )

        # æ ‡å‡†åŒ–
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        X_train_scaled = scaler_X.fit_transform(X_train)
        X_test_scaled = scaler_X.transform(X_test)
        y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()

        # 5. å¤šæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
        models = {
            'Ridgeå›å½’': Ridge(alpha=1.0, random_state=42),
            'Lassoå›å½’': Lasso(alpha=0.1, random_state=42),
            'æ¢¯åº¦æå‡': GradientBoostingRegressor(
                n_estimators=50,
                max_depth=3,
                learning_rate=0.1,
                random_state=42,
                subsample=0.8
            ),
            'æ”¯æŒå‘é‡æœº': SVR(kernel='linear', C=1.0, epsilon=0.1),
        }

        results = {}
        for name, model in models.items():
            # è®­ç»ƒæ¨¡å‹
            model.fit(X_train_scaled, y_train_scaled)

            # é¢„æµ‹
            y_pred_scaled = model.predict(X_test_scaled)
            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
            y_true = y_test.values

            # è®­ç»ƒé›†é¢„æµ‹ï¼ˆç”¨äºæ£€æµ‹è¿‡æ‹Ÿåˆï¼‰
            y_train_pred_scaled = model.predict(X_train_scaled)
            y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()

            # è®¡ç®—æŒ‡æ ‡
            r2_train = r2_score(y_train, y_train_pred)
            r2_test = r2_score(y_true, y_pred)
            rmse_test = np.sqrt(mean_squared_error(y_true, y_pred))
            mae_test = mean_absolute_error(y_true, y_pred)

            results[name] = {
                'model': model,
                'r2_train': float(r2_train),
                'r2_test': float(r2_test),
                'rmse_test': float(rmse_test),
                'mae_test': float(mae_test),
                'overfitting_gap': float(r2_train - r2_test),
                'y_pred': y_pred.tolist(),
                'y_true': y_true.tolist()
            }

        # 6. é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºæµ‹è¯•é›†RÂ²ï¼ŒåŒæ—¶æƒ©ç½šè¿‡æ‹Ÿåˆï¼‰
        best_model_name = None
        best_score = -float('inf')

        for name, result in results.items():
            # æƒ©ç½šè¿‡æ‹Ÿåˆä¸¥é‡çš„æ¨¡å‹
            penalty = 0
            if result['overfitting_gap'] > 0.2:
                penalty = 0.3  # ä¸¥é‡è¿‡æ‹Ÿåˆ
            elif result['overfitting_gap'] > 0.1:
                penalty = 0.1  # è½»å¾®è¿‡æ‹Ÿåˆ

            score = result['r2_test'] - penalty

            if score > best_score:
                best_score = score
                best_model_name = name

        best_result = results[best_model_name]
        best_model = best_result['model']

        # 7. ç‰¹å¾é‡è¦æ€§
        importance_df = None
        if hasattr(best_model, 'coef_'):
            # çº¿æ€§æ¨¡å‹
            importance = best_model.coef_
            if isinstance(importance, np.ndarray):
                importance = importance.tolist()

            importance_df = pd.DataFrame({
                "feature_name": feature_cols,
                "importance": importance,
                "abs_importance": [abs(x) for x in importance]
            }).sort_values('abs_importance', ascending=False)

        elif hasattr(best_model, 'feature_importances_'):
            # æ ‘æ¨¡å‹
            importance = best_model.feature_importances_.tolist()
            importance_df = pd.DataFrame({
                "feature_name": feature_cols,
                "importance": importance,
                "abs_importance": importance
            }).sort_values('abs_importance', ascending=False)

        # 8. æ ¼å¼åŒ–é¢„æµ‹å€¼
        def format_prediction_value(value, original_series):
            """æ ¼å¼åŒ–é¢„æµ‹å€¼"""
            try:
                if original_series.dropna().apply(lambda x: float(x).is_integer()).all():
                    return int(round(value))
                else:
                    return round(float(value), 4)
            except:
                return round(float(value), 4)

        # 9. è·å–æœ€ä½³æ¨¡å‹çš„é¢„æµ‹ç»“æœ
        y_pred = np.array(best_result['y_pred'])
        y_true = np.array(best_result['y_true'])

        # 10. æ„å»ºç»“æœå­—å…¸
        output_results = {
            "metadata": {
                "target_column": target_column,
                "model_used": best_model_name,
                "feature_columns": feature_cols,
                "total_samples": int(len(df)),
                "valid_samples": int(len(df_clean)),
                "train_samples": int(len(X_train)),
                "test_samples": int(len(X_test)),
                "performance_metrics": {
                    "R2_train": float(best_result['r2_train']),
                    "R2_test": float(best_result['r2_test']),
                    "RMSE_test": float(best_result['rmse_test']),
                    "MAE_test": float(best_result['mae_test']),
                    "overfitting_gap": float(best_result['overfitting_gap'])
                },
                "model_selection_note": "é€‰æ‹©æ ‡å‡†ï¼šæµ‹è¯•é›†RÂ²æœ€é«˜ï¼ŒåŒæ—¶æƒ©ç½šè¿‡æ‹Ÿåˆä¸¥é‡çš„æ¨¡å‹"
            },
            "model_comparison": {},
            "predictions": [],
            "feature_importance": []
        }

        # æ·»åŠ æ¨¡å‹æ¯”è¾ƒä¿¡æ¯
        for name, result in results.items():
            output_results["model_comparison"][name] = {
                "R2_train": float(result['r2_train']),
                "R2_test": float(result['r2_test']),
                "overfitting_gap": float(result['overfitting_gap']),
                "selected": bool(name == best_model_name)
            }

        # å¡«å……é¢„æµ‹ç»“æœ
        for i, idx in enumerate(test_idx):
            # è·å–é¡¹ç›®åç§°
            proj_name = ""
            for name_col in ['projectname', 'projectname2']:
                if name_col in df_clean.columns and pd.notna(df_clean.loc[idx, name_col]):
                    proj_name = str(df_clean.loc[idx, name_col])
                    break
            if not proj_name:
                proj_name = f"é¡¹ç›®_{idx}"

            # æ ¼å¼åŒ–å€¼
            true_val = format_prediction_value(y_true[i], df[target_column])
            pred_val = format_prediction_value(y_pred[i], df[target_column])

            absolute_error = abs(float(y_true[i]) - float(y_pred[i]))
            relative_error = abs((float(y_true[i]) - float(y_pred[i])) / (abs(float(y_true[i])) + 1e-8)) * 100

            output_results["predictions"].append({
                "project_name": proj_name,
                "true_value": float(true_val) if isinstance(true_val, (int, float)) else true_val,
                "predicted_value": float(pred_val) if isinstance(pred_val, (int, float)) else pred_val,
                "absolute_error": round(float(absolute_error), 4),
                "relative_error_percent": round(float(relative_error), 2)
            })

        # æ·»åŠ ç‰¹å¾é‡è¦æ€§
        if importance_df is not None:
            for _, row in importance_df.iterrows():
                output_results["feature_importance"].append({
                    "feature_name": str(row['feature_name']),
                    "importance": float(row['importance']),
                    "abs_importance": float(row['abs_importance'])
                })

        # 11. è¿”å›ç»“æœ
        return output_results

    except Exception as e:
        raise Exception(f"é¢„æµ‹å¤±è´¥: {str(e)}")


# ==================== å‘½ä»¤è¡Œè„šæœ¬æ¨¡å¼ ====================
# åªæœ‰ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ—¶æ‰ä¼šæ‰§è¡Œä»¥ä¸‹ä»£ç ï¼Œè¢«å¯¼å…¥æ—¶ä¸ä¼šæ‰§è¡Œ
if __name__ == "__main__":
    # ==================== 1. åŠ è½½CSVæ•°æ® ====================
    print("ã€1/8ã€‘åŠ è½½CSVæ•°æ®...")
    csv_path = r"C:\Users\22390\Desktop\OpenSODA\backendData\top_300_metrics.csv"

    def load_csv_data(file_path):
        """åŠ è½½CSVæ–‡ä»¶"""
        try:
            df = pd.read_csv(file_path)
            print(f"  âœ“ æˆåŠŸåŠ è½½æ–‡ä»¶: {file_path}")
            print(f"  âœ“ æ•°æ®å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
            print(f"  âœ“ åˆ—ååˆ—è¡¨: {list(df.columns)}")
            print(f"  âœ“ å‰5è¡Œé¢„è§ˆ:")
            print(df.head())
            return df
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
            print("è¯·ç¡®è®¤æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚å½“å‰è·¯å¾„ä¸‹çš„æ–‡ä»¶åˆ—è¡¨:")
            folder = os.path.dirname(file_path)
            if os.path.exists(folder):
                for f in os.listdir(folder):
                    if f.endswith('.csv'):
                        print(f"  - {f}")
            exit()
        except Exception as e:
            print(f"âŒ åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
            exit()

    try:
        df = load_csv_data(csv_path)
    except Exception as e:
        print(e)
        exit()
    # ==================== 2. é€‰æ‹©ç›®æ ‡åˆ—å¹¶å¤„ç†ç±»å‹ ====================
    print("\nã€2/8ã€‘é€‰æ‹©å¹¶å¤„ç†ç›®æ ‡åˆ—...")
    # å±•ç¤ºåˆ—åä¾›é€‰æ‹©
    cols = list(df.columns)
    for idx, col in enumerate(cols):
        print(f"  [{idx}] {col}")

    # é€‰æ‹©ç›®æ ‡åˆ—
    target_column = None
    while not target_column:
        user_input = input("\nè¾“å…¥ç›®æ ‡åˆ—åºå·/åˆ—åï¼š").strip()
        if user_input.isdigit():
            idx = int(user_input)
            if 0 <= idx < len(cols):
                target_column = cols[idx]
            else:
                print(f"âŒ åºå·è¶…å‡ºèŒƒå›´ï¼ˆ0-{len(cols)-1}ï¼‰")
        elif user_input in cols:
            target_column = user_input
        else:
            print(f"âŒ åˆ—å '{user_input}' ä¸å­˜åœ¨")

    print(f"âœ“ é€‰ä¸­ç›®æ ‡åˆ—ï¼š{target_column}")

    # å¤„ç†ç›®æ ‡åˆ—ç±»å‹
    def convert_to_numeric(col_data):
        """æ™ºèƒ½è½¬æ¢ä¸ºæ•°å€¼å‹ï¼ˆå…¼å®¹æ—¶é—´/æ•°å­—ï¼‰"""
        # å°è¯•è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆå¦‚æœæ˜¯æ—¶é—´å­—ç¬¦ä¸²ï¼‰
        try:
            # å¸¸è§æ—¶é—´æ ¼å¼åŒ¹é…
            time_formats = ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S', '%Y/%m/%d', '%Y%m%d']
            for fmt in time_formats:
                try:
                    return pd.to_datetime(col_data, format=fmt).astype('int64') // 10**9  # è½¬ç§’çº§æ—¶é—´æˆ³
                except:
                    continue
            # è‡ªåŠ¨è¯†åˆ«æ—¶é—´æ ¼å¼
            return pd.to_datetime(col_data).astype('int64') // 10**9
        except:
            # è½¬æ¢ä¸ºæ™®é€šæ•°å€¼
            return pd.to_numeric(col_data, errors='coerce')

    # è½¬æ¢ç›®æ ‡åˆ—å¹¶æ¸…ç†ç©ºå€¼
    df['target_numeric'] = convert_to_numeric(df[target_column])
    df_clean = df.dropna(subset=['target_numeric']).reset_index(drop=True)
    print(f"âœ“ ç›®æ ‡åˆ—å¤„ç†å®Œæˆï¼š{len(df_clean)} æ¡æœ‰æ•ˆæ•°æ®")
    if len(df_clean) == 0:
        print("âŒ ç›®æ ‡åˆ—æ— æœ‰æ•ˆæ•°å€¼æ•°æ®")
        exit()

    # ==================== 3. ç”ŸæˆåŸºç¡€ç‰¹å¾ï¼ˆæ— åŸå§‹ç‰¹å¾æ—¶ï¼‰ ====================
    print("\nã€3/8ã€‘ç”Ÿæˆç‰¹å¾...")
    # è‡ªåŠ¨ç”ŸæˆåŸºç¡€ç‰¹å¾ï¼ˆåŸºäºç°æœ‰åˆ—ï¼‰
    feature_cols = []

    # 1. å¯¹æ‰€æœ‰éç›®æ ‡åˆ—å°è¯•è½¬ä¸ºæ•°å€¼ç‰¹å¾
    for col in df_clean.columns:
        if col not in [target_column, 'target_numeric']:
            try:
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
                df_clean[f'feat_{col}'] = pd.to_numeric(df_clean[col], errors='coerce')
                # å°è¯•è½¬æ¢æ—¶é—´ä¸ºæ—¶é—´æˆ³
                if pd.isna(df_clean[f'feat_{col}']).all():
                    df_clean[f'feat_{col}'] = convert_to_numeric(df_clean[col])
                # ç§»é™¤å…¨ç©ºç‰¹å¾
                if not pd.isna(df_clean[f'feat_{col}']).all():
                    df_clean[f'feat_{col}'] = df_clean[f'feat_{col}'].fillna(0)
                    feature_cols.append(f'feat_{col}')
            except:
                continue

    # 2. å¦‚æœæ— ä»»ä½•ç‰¹å¾ï¼Œç”Ÿæˆç®€å•åºåˆ—ç‰¹å¾
    if not feature_cols:
        print("âš ï¸  æ— æœ‰æ•ˆç‰¹å¾åˆ—ï¼Œç”Ÿæˆåºåˆ—ç‰¹å¾")
        df_clean['feat_index'] = df_clean.index  # è¡Œç´¢å¼•ç‰¹å¾
        df_clean['feat_random'] = np.random.rand(len(df_clean))  # éšæœºç‰¹å¾ï¼ˆå…œåº•ï¼‰
        feature_cols = ['feat_index', 'feat_random']

    print(f"âœ“ ç”Ÿæˆç‰¹å¾ï¼š{feature_cols}")

    # ==================== 4. æ•°æ®å‡†å¤‡ ====================
    print("\nã€4/8ã€‘æ•°æ®æ‹†åˆ†ä¸æ ‡å‡†åŒ–...")
    X = df_clean[feature_cols]
    y = df_clean['target_numeric']

    # æ‹†åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test, train_idx, test_idx = train_test_split(
        X, y, df_clean.index, test_size=0.3, random_state=42
    )

    # æ ‡å‡†åŒ–
    scaler_X = StandardScaler()
    scaler_y = StandardScaler()
    X_train_scaled = scaler_X.fit_transform(X_train)
    X_test_scaled = scaler_X.transform(X_test)
    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()

    print(f"âœ“ æ‹†åˆ†å®Œæˆï¼šè®­ç»ƒé›† {len(X_train)} æ¡ï¼Œæµ‹è¯•é›† {len(X_test)} æ¡")

    # ==================== 5. å¤šæ¨¡å‹è®­ç»ƒä¸è¯„ä¼° ====================
    print("\nã€5/8ã€‘å¤šæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°...")

    # å®šä¹‰æ¨¡å‹
    models = {
        'Ridgeå›å½’': Ridge(alpha=1.0, random_state=42),
        'Lassoå›å½’': Lasso(alpha=0.1, random_state=42),
        'æ¢¯åº¦æå‡': GradientBoostingRegressor(
            n_estimators=50,
            max_depth=3,
            learning_rate=0.1,
            random_state=42,
            subsample=0.8
        ),
        'æ”¯æŒå‘é‡æœº': SVR(kernel='linear', C=1.0, epsilon=0.1),
    }

    # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
    results = {}
    for name, model in models.items():
        print(f"\n  è®­ç»ƒ {name}...")

        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train_scaled, y_train_scaled)

        # é¢„æµ‹
        y_pred_scaled = model.predict(X_test_scaled)
        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
        y_true = y_test.values

        # è®­ç»ƒé›†é¢„æµ‹ï¼ˆç”¨äºæ£€æµ‹è¿‡æ‹Ÿåˆï¼‰
        y_train_pred_scaled = model.predict(X_train_scaled)
        y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1)).flatten()

        # è®¡ç®—æŒ‡æ ‡
        r2_train = r2_score(y_train, y_train_pred)
        r2_test = r2_score(y_true, y_pred)
        rmse_test = np.sqrt(mean_squared_error(y_true, y_pred))
        mae_test = mean_absolute_error(y_true, y_pred)

        results[name] = {
            'model': model,
            'r2_train': float(r2_train),
            'r2_test': float(r2_test),
            'rmse_test': float(rmse_test),
            'mae_test': float(mae_test),
            'overfitting_gap': float(r2_train - r2_test),
            'y_pred': y_pred.tolist(),
            'y_true': y_true.tolist()
        }

        print(f"    è®­ç»ƒé›†RÂ²: {r2_train:.4f}")
        print(f"    æµ‹è¯•é›†RÂ²: {r2_test:.4f}")
        print(f"    è¿‡æ‹Ÿåˆå·®è·: {r2_train - r2_test:.4f}")
        print(f"    RMSE: {rmse_test:.2e}")
        print(f"    MAE: {mae_test:.2e}")

    # é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºæµ‹è¯•é›†RÂ²ï¼ŒåŒæ—¶è€ƒè™‘è¿‡æ‹Ÿåˆï¼‰
    best_model_name = None
    best_score = -float('inf')

    for name, result in results.items():
        # æƒ©ç½šè¿‡æ‹Ÿåˆä¸¥é‡çš„æ¨¡å‹
        penalty = 0
        if result['overfitting_gap'] > 0.2:
            penalty = 0.3  # ä¸¥é‡è¿‡æ‹Ÿåˆï¼Œå¤§å¹…æƒ©ç½š
        elif result['overfitting_gap'] > 0.1:
            penalty = 0.1  # è½»å¾®è¿‡æ‹Ÿåˆï¼Œè½»å¾®æƒ©ç½š

        score = result['r2_test'] - penalty

        if score > best_score:
            best_score = score
            best_model_name = name

    print(f"\nâœ“ é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼š{best_model_name}")
    best_result = results[best_model_name]
    best_model = best_result['model']

    # ==================== 6. ç‰¹å¾é‡è¦æ€§ ====================
    print("\nã€6/8ã€‘ç‰¹å¾é‡è¦æ€§åˆ†æ...")

    importance_df = None
    if hasattr(best_model, 'coef_'):
        # çº¿æ€§æ¨¡å‹
        importance = best_model.coef_
        if isinstance(importance, np.ndarray):
            importance = importance.tolist()

        importance_df = pd.DataFrame({
            "feature_name": feature_cols,
            "importance": importance,
            "abs_importance": [abs(x) for x in importance]
        }).sort_values('abs_importance', ascending=False)

    elif hasattr(best_model, 'feature_importances_'):
        # æ ‘æ¨¡å‹
        importance = best_model.feature_importances_.tolist()
        importance_df = pd.DataFrame({
            "feature_name": feature_cols,
            "importance": importance,
            "abs_importance": importance
        }).sort_values('abs_importance', ascending=False)

    if importance_df is not None:
        print(f"\nğŸ“ˆ ç‰¹å¾é‡è¦æ€§æ’åï¼š")
        for i, row in importance_df.iterrows():
            print(f"  {row['feature_name']:30s}: {row['importance']:.6f}")
    else:
        print("âš ï¸  è¯¥æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")

    # ==================== 7. ç”Ÿæˆé¢„æµ‹ç»“æœ ====================
    print("\nã€7/8ã€‘ç”Ÿæˆé¢„æµ‹ç»“æœ...")

    def format_prediction_value(value, original_series):
        """æ ¼å¼åŒ–é¢„æµ‹å€¼"""
        try:
            if original_series.dropna().apply(lambda x: float(x).is_integer()).all():
                return int(round(value))
            else:
                return round(float(value), 4)
        except:
            return round(float(value), 4)

    # è·å–æœ€ä½³æ¨¡å‹çš„é¢„æµ‹ç»“æœ
    y_pred = np.array(best_result['y_pred'])
    y_true = np.array(best_result['y_true'])

    # æ„å»ºç»“æœå­—å…¸ - ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
    output_results = {
        "metadata": {
            "target_column": target_column,
            "model_used": best_model_name,
            "feature_columns": feature_cols,
            "total_samples": int(len(df)),
            "valid_samples": int(len(df_clean)),
            "train_samples": int(len(X_train)),
            "test_samples": int(len(X_test)),
            "performance_metrics": {
                "R2_train": float(best_result['r2_train']),
                "R2_test": float(best_result['r2_test']),
                "RMSE_test": float(best_result['rmse_test']),
                "MAE_test": float(best_result['mae_test']),
                "overfitting_gap": float(best_result['overfitting_gap'])
            },
            "model_selection_note": "é€‰æ‹©æ ‡å‡†ï¼šæµ‹è¯•é›†RÂ²æœ€é«˜ï¼ŒåŒæ—¶æƒ©ç½šè¿‡æ‹Ÿåˆä¸¥é‡çš„æ¨¡å‹"
        },
        "model_comparison": {},
        "predictions": []
    }

    # æ·»åŠ æ¨¡å‹æ¯”è¾ƒä¿¡æ¯
    for name, result in results.items():
        output_results["model_comparison"][name] = {
            "R2_train": float(result['r2_train']),
            "R2_test": float(result['r2_test']),
            "overfitting_gap": float(result['overfitting_gap']),
            "selected": bool(name == best_model_name)
        }

    # å¡«å……é¢„æµ‹ç»“æœ
    for i, idx in enumerate(test_idx):
        # è·å–é¡¹ç›®åç§°
        proj_name = ""
        for name_col in ['projectname', 'projectname2']:
            if name_col in df_clean.columns and pd.notna(df_clean.loc[idx, name_col]):
                proj_name = str(df_clean.loc[idx, name_col])
                break
        if not proj_name:
            proj_name = f"é¡¹ç›®_{idx}"

        # æ ¼å¼åŒ–å€¼
        true_val = format_prediction_value(y_true[i], df[target_column])
        pred_val = format_prediction_value(y_pred[i], df[target_column])

        absolute_error = abs(float(y_true[i]) - float(y_pred[i]))
        relative_error = abs((float(y_true[i]) - float(y_pred[i])) / (abs(float(y_true[i])) + 1e-8)) * 100

        output_results["predictions"].append({
            "project_name": proj_name,
            "true_value": float(true_val) if isinstance(true_val, (int, float)) else true_val,
            "predicted_value": float(pred_val) if isinstance(pred_val, (int, float)) else pred_val,
            "absolute_error": round(float(absolute_error), 4),
            "relative_error_percent": round(float(relative_error), 2)
        })

    # ==================== 8. ä¿å­˜ç»“æœ ====================
    print("\nã€8/8ã€‘ä¿å­˜ç»“æœ...")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = r'C:\Users\22390\Desktop\OpenSODA\backendData'
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜é¢„æµ‹ç»“æœ
    output_pred = os.path.join(output_dir, 'prediction_results_fixed.json')
    with open(output_pred, 'w', encoding='utf-8') as f:
        json.dump(output_results, f, ensure_ascii=False, indent=4)

    print(f"âœ“ é¢„æµ‹ç»“æœï¼š{output_pred}")

    # ä¿å­˜ç‰¹å¾é‡è¦æ€§
    if importance_df is not None:
        # è½¬æ¢importance_dfä¸­çš„å€¼ä¸ºPythonåŸç”Ÿç±»å‹
        importance_records = []
        for _, row in importance_df.iterrows():
            record = {
                "feature_name": str(row['feature_name']),
                "importance": float(row['importance']),
                "abs_importance": float(row['abs_importance'])
            }
            importance_records.append(record)

        importance_dict = {
            "model": best_model_name,
            "feature_importance": importance_records
        }
        output_imp = os.path.join(output_dir, 'feature_importance_fixed.json')
        with open(output_imp, 'w', encoding='utf-8') as f:
            json.dump(importance_dict, f, ensure_ascii=False, indent=4)
        print(f"âœ“ ç‰¹å¾é‡è¦æ€§ï¼š{output_imp}")

    # ä¿å­˜æ¨¡å‹è¯„ä¼°æ‘˜è¦
    summary = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "target_column": target_column,
        "best_model": best_model_name,
        "test_r2": float(best_result['r2_test']),
        "test_rmse": float(best_result['rmse_test']),
        "overfitting_gap": float(best_result['overfitting_gap']),
        "is_overfitted": bool(best_result['overfitting_gap'] > 0.15),
        "features_used": int(len(feature_cols))
    }
    output_summary = os.path.join(output_dir, 'model_summary_fixed.json')
    with open(output_summary, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=4)

    print(f"âœ“ æ¨¡å‹æ‘˜è¦ï¼š{output_summary}")

    # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    print(f"\nğŸ“‹ æœ€ç»ˆæŠ¥å‘Šï¼š")
    print(f"  æœ€ä½³æ¨¡å‹: {best_model_name}")
    print(f"  æµ‹è¯•é›†RÂ²: {best_result['r2_test']:.4f}")
    print(f"  è®­ç»ƒé›†RÂ²: {best_result['r2_train']:.4f}")
    print(f"  è¿‡æ‹Ÿåˆå·®è·: {best_result['overfitting_gap']:.4f}")
    print(f"  æ˜¯å¦è¿‡æ‹Ÿåˆ: {'æ˜¯' if best_result['overfitting_gap'] > 0.15 else 'å¦'}")

    # è¾“å‡ºæ‰€æœ‰æ¨¡å‹æ¯”è¾ƒ
    print(f"\nğŸ“Š æ¨¡å‹æ¯”è¾ƒï¼š")
    print(f"  {'æ¨¡å‹åç§°':<15} {'è®­ç»ƒRÂ²':<10} {'æµ‹è¯•RÂ²':<10} {'è¿‡æ‹Ÿåˆå·®è·':<12} {'é€‰æ‹©'}")
    print("-" * 60)
    for name, result in results.items():
        selected = "âœ“" if name == best_model_name else ""
        print(f"  {name:<15} {result['r2_train']:.4f}     {result['r2_test']:.4f}     {result['overfitting_gap']:.4f}        {selected}")

    # è§£é‡Šç»“æœ
    print(f"\nğŸ“ ç»“æœè§£é‡Šï¼š")
    if best_result['r2_test'] > 0.7:
        print("  âœ… æ¨¡å‹è¡¨ç°ä¼˜ç§€")
    elif best_result['r2_test'] > 0.5:
        print("  âš ï¸  æ¨¡å‹è¡¨ç°ä¸€èˆ¬")
    elif best_result['r2_test'] > 0.3:
        print("  âš ï¸  æ¨¡å‹è¡¨ç°è¾ƒå·®")
    else:
        print("  âŒ æ¨¡å‹è¡¨ç°éå¸¸å·®")

    if best_result['overfitting_gap'] > 0.2:
        print("  âŒ ä¸¥é‡è¿‡æ‹Ÿåˆï¼šå»ºè®®å‡å°‘ç‰¹å¾æˆ–å¢åŠ æ­£åˆ™åŒ–")
    elif best_result['overfitting_gap'] > 0.1:
        print("  âš ï¸  è½»å¾®è¿‡æ‹Ÿåˆï¼šæ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸€èˆ¬")
    else:
        print("  âœ… è¿‡æ‹Ÿåˆé£é™©ä½ï¼šæ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½")

    # æ£€æŸ¥æ˜¯å¦ä»ç„¶è¿‡æ‹Ÿåˆ
    if best_result['r2_test'] > 0.95:
        print(f"\nâš ï¸  è­¦å‘Šï¼šæ¨¡å‹å¯èƒ½ä»ç„¶è¿‡æ‹Ÿåˆæˆ–å­˜åœ¨æ•°æ®æ³„æ¼ï¼")
        print(f"  å¯èƒ½åŸå› ï¼š")
        print(f"  1. ç‰¹å¾ä¸ç›®æ ‡åˆ—é«˜åº¦ç›¸å…³")
        print(f"  2. æ•°æ®é‡å¤ªå°ï¼ˆä»…{len(df_clean)}æ¡ï¼‰")
        print(f"  3. ç‰¹å¾å·¥ç¨‹éœ€è¦è°ƒæ•´")