import pandas as pd
import numpy as np
import json
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')


# ==================== å°è£…çš„é¢„æµ‹å‡½æ•° ====================
def predict_response_time(csv_path: str = r'C:\Users\22390\Desktop\OpenSODA\backendData\top_300_metrics.csv',
                         progress_callback=None) -> dict:
    """
    é¢„æµ‹ Change Request å“åº”æ—¶é—´ï¼ˆæ”¯æŒè¿›åº¦å›è°ƒï¼‰

    å‚æ•°:
        csv_path: CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œæœ‰é»˜è®¤å€¼ï¼‰
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (progress, message) å‚æ•°
                          progress: 0-100 çš„æ•´æ•°
                          message: å½“å‰æ­¥éª¤æè¿°

    è¿”å›:
        åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        {
            "metadata": {...},           # å…ƒæ•°æ®ä¿¡æ¯
            "model_evaluation": {...},   # æ¨¡å‹è¯„ä¼°æŒ‡æ ‡
            "future_prediction": {...},  # æœªæ¥é¢„æµ‹ç»“æœ
            "historical_data_sample": [...] # å†å²æ•°æ®æ ·æœ¬
        }
    """
    def update_progress(progress, message):
        """æ›´æ–°è¿›åº¦"""
        if progress_callback:
            progress_callback(progress, message)

    try:
        # ã€1/7ã€‘åŠ è½½æ•°æ®
        update_progress(14, "ã€1/7ã€‘åŠ è½½æ•°æ®...")
        df = pd.read_csv(csv_path)

        # ã€2/7ã€‘è·³è¿‡æ—¶åºå¯è§†åŒ–ï¼ˆä¸ç”Ÿæˆå›¾ç‰‡ï¼‰
        update_progress(28, "ã€2/7ã€‘è§£ææ—¶åºæ•°æ®...")

        # ã€3/7ã€‘æ„å»ºé¢„æµ‹æ•°æ®é›†
        update_progress(42, "ã€3/7ã€‘æ„å»ºé¢„æµ‹æ•°æ®é›†...")
        pred_data = []
        project_meta = {}

        for idx, row in df.iterrows():
            proj_id = idx
            proj_name = row.get('projectname2', f'é¡¹ç›®_{idx}')
            project_meta[proj_id] = proj_name

            # è§£æå“åº”æ—¶é—´æ•°æ®
            times, values = parse_time_series_dict(row.get('change_request_response_time', ''))
            if len(times) < 3:
                continue

            # æ³¨æ„ï¼štimes ä¸­çš„å…ƒç´ å·²ç»æ˜¯ '2022-08' è¿™æ ·çš„å­—ç¬¦ä¸²æ ¼å¼
            for time_str, response_time in zip(times, values):
                time_features = time_to_features(time_str)
                pred_data.append({
                    'project_id': proj_id,
                    'time_str': time_str,
                    'response_time': response_time,
                    **time_features
                })

        pred_df = pd.DataFrame(pred_data)
        pred_df = add_temporal_features(pred_df)

        # ã€4/7ã€‘æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†
        update_progress(56, "ã€4/7ã€‘æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†...")
        y = pred_df['response_time'].values
        Q1, Q3 = np.percentile(y, [25, 75])
        IQR = Q3 - Q1
        mask = (y >= Q1 - 2 * IQR) & (y <= Q3 + 2 * IQR)
        pred_df_clean = pred_df[mask].reset_index(drop=True)

        # ç‰¹å¾å’Œç›®æ ‡
        feature_cols = ['year', 'month', 'quarter', 'month_order', 'is_quarter_end',
                       'is_year_end', 'is_peak_season', 'month_sin', 'month_cos',
                       'response_time_ma_3', 'response_time_ma_6',
                       'response_time_std_3', 'response_time_std_6',
                       'response_time_diff_1', 'response_time_lag_1', 'response_time_lag_2']

        X = pred_df_clean[feature_cols].values
        y = pred_df_clean['response_time'].values

        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # æ ‡å‡†åŒ–
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        # ã€5/7ã€‘æ¨¡å‹è®­ç»ƒä¸è°ƒä¼˜
        update_progress(70, "ã€5/7ã€‘æ¨¡å‹è®­ç»ƒä¸è°ƒä¼˜...")

        # XGBoost æ¨¡å‹
        xgb_model = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=5,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
        xgb_model.fit(
            X_train_scaled, y_train,
            eval_set=[(X_test_scaled, y_test)],
            verbose=False
        )

        # é¢„æµ‹
        y_pred_train = xgb_model.predict(X_train_scaled)
        y_pred_test = xgb_model.predict(X_test_scaled)

        # è¯„ä¼°æŒ‡æ ‡
        r2_train = r2_score(y_train, y_pred_train)
        r2_test = r2_score(y_test, y_pred_test)
        mae = mean_absolute_error(y_test, y_pred_test)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

        # äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=5)
        cv_scores = cross_val_score(xgb_model, X_train_scaled, y_train, cv=tscv, scoring='r2')

        # MAPE
        mape = np.mean(np.abs((y_test - y_pred_test) / (y_test + 1e-10))) * 100

        # ã€6/7ã€‘æ¨¡å‹è¯„ä¼°ä¸æœªæ¥é¢„æµ‹
        update_progress(85, "ã€6/7ã€‘æ¨¡å‹è¯„ä¼°ä¸æœªæ¥é¢„æµ‹...")

        # æœªæ¥é¢„æµ‹ï¼ˆæœªæ¥6ä¸ªæœˆï¼‰
        last_time_str = pred_df_clean['time_str'].iloc[-1]
        last_year, last_month = map(int, last_time_str.split('-'))

        future_predictions = []
        future_time_labels = []

        for i in range(1, 7):
            future_month = last_month + i
            future_year = last_year
            while future_month > 12:
                future_month -= 12
                future_year += 1

            future_time_str = f'{future_year}-{future_month:02d}'
            future_time_labels.append(future_time_str)

            # æ„å»ºæœªæ¥ç‰¹å¾
            future_features = time_to_features(future_time_str)
            future_X = np.array([[
                future_features['year'],
                future_features['month'],
                future_features['quarter'],
                future_features['month_order'],
                future_features['is_quarter_end'],
                future_features['is_year_end'],
                future_features['is_peak_season'],
                future_features['month_sin'],
                future_features['month_cos'],
                y[-1],  # ä½¿ç”¨æœ€åçš„å“åº”æ—¶é—´ä½œä¸ºç§»åŠ¨å¹³å‡
                y[-1],
                0, 0, 0,
                y[-1],
                y[-2] if len(y) > 1 else y[-1]
            ]])

            future_X_scaled = scaler.transform(future_X)
            future_pred = xgb_model.predict(future_X_scaled)[0]
            future_predictions.append(round(float(future_pred), 2))

        # ã€7/7ã€‘ä¿å­˜JSONæ ¼å¼ç»“æœ
        update_progress(100, "ã€7/7ã€‘ä¿å­˜JSONæ ¼å¼ç»“æœ...")

        # æ„å»ºè¿”å›ç»“æœ
        result = {
            "metadata": {
                "data_source": "top_300_metrics.csv",
                "target_metric": "change_request_response_time",
                "total_projects": len(df),
                "valid_samples": len(pred_df_clean),
                "feature_columns": feature_cols,
                "best_model": "XGBoost"
            },
            "model_evaluation": {
                "XGBoost": {
                    "r2_train": round(r2_train, 4),
                    "r2_test": round(r2_test, 4),
                    "mae": round(mae, 2),
                    "rmse": round(rmse, 2),
                    "cv_mean": round(cv_scores.mean(), 4),
                    "cv_std": round(cv_scores.std(), 4),
                    "best_params": {
                        "n_estimators": 200,
                        "max_depth": 5,
                        "learning_rate": 0.05
                    },
                    "mape": round(mape, 2)
                }
            },
            "future_prediction": {
                "prediction_time_points": future_time_labels,
                "predicted_response_time": future_predictions,
                "prediction_explanation": "é¢„æµ‹æœªæ¥6ä¸ªæœˆçš„Change Requestå“åº”æ—¶é—´ï¼ˆåŸºäºæœ€ä¼˜XGBoostæ¨¡å‹ï¼‰"
            },
            "historical_data_sample": []
        }

        # æ·»åŠ å†å²æ•°æ®æ ·æœ¬ï¼ˆæœ€è¿‘20æ¡ï¼‰
        sample_data = pred_df_clean.tail(20)
        for _, row in sample_data.iterrows():
            result["historical_data_sample"].append({
                "time_str": row['time_str'],
                "response_time": round(float(row['response_time']), 2),
                "year": int(row['year']),
                "month": int(row['month'])
            })

        update_progress(100, "å®Œæˆï¼")
        return result

    except Exception as e:
        raise Exception(f"é¢„æµ‹å¤±è´¥: {str(e)}")


# ==================== å·¥å…·å‡½æ•° ====================
def parse_time_series_dict(dict_str):
    """è§£ææ—¶åºå­—å…¸ï¼Œè¿”å›(æ—¶é—´åˆ—è¡¨, å€¼åˆ—è¡¨)"""
    try:
        if pd.isna(dict_str) or dict_str == '':
            return [], []
        data_dict = eval(dict_str)
        # æŒ‰æ—¶é—´æ’åº
        sorted_items = sorted(data_dict.items(), key=lambda x: x[0])
        times = [item[0] for item in sorted_items]
        values = [float(item[1]) for item in sorted_items]
        return times, values
    except:
        return [], []

def time_to_features(time_str):
    """æ—¶é—´å­—ç¬¦ä¸²åˆ†è§£ä¸ºç‰¹å¾ï¼šå¹´ã€æœˆã€å­£åº¦ã€æœˆä»½åºã€æ˜¯å¦å¹´æœ«/å­£åº¦æœ«ç­‰"""
    year, month = map(int, time_str.split('-'))
    quarter = (month - 1) // 3 + 1
    month_order = (year - 2015) * 12 + month  # ä»¥2015å¹´ä¸ºåŸºå‡†çš„ç´¯è®¡æœˆä»½
    is_quarter_end = 1 if month in [3,6,9,12] else 0
    is_year_end = 1 if month == 12 else 0
    is_peak_season = 1 if month in [1,2,9,10,11,12] else 0  # ä¸šåŠ¡é«˜å³°æœŸ
    month_sin = np.sin(2 * np.pi * month / 12)  # æœˆä»½å‘¨æœŸæ€§ç‰¹å¾
    month_cos = np.cos(2 * np.pi * month / 12)
    return {
        'year': year,
        'month': month,
        'quarter': quarter,
        'month_order': month_order,
        'is_quarter_end': is_quarter_end,
        'is_year_end': is_year_end,
        'is_peak_season': is_peak_season,
        'month_sin': month_sin,
        'month_cos': month_cos
    }

def add_temporal_features(df):
    """æ·»åŠ æ—¶åºè¡ç”Ÿç‰¹å¾ï¼šç§»åŠ¨å¹³å‡ã€å·®åˆ†ã€æ»åç‰¹å¾"""
    # æŒ‰é¡¹ç›®å’Œæ—¶é—´æ’åº
    df = df.sort_values(['project_id', 'month_order']).reset_index(drop=True)

    # åˆ†ç»„æ·»åŠ ç‰¹å¾ï¼ˆæŒ‰é¡¹ç›®ï¼‰
    for window in [3, 6]:
        # ç§»åŠ¨å¹³å‡
        df[f'response_time_ma_{window}'] = df.groupby('project_id')['response_time'].transform(
            lambda x: x.rolling(window=window, min_periods=1).mean()
        )
        # ç§»åŠ¨æ ‡å‡†å·®
        df[f'response_time_std_{window}'] = df.groupby('project_id')['response_time'].transform(
            lambda x: x.rolling(window=window, min_periods=1).std().fillna(0)
        )

    # ä¸€é˜¶å·®åˆ†ï¼ˆå˜åŒ–è¶‹åŠ¿ï¼‰
    df['response_time_diff_1'] = df.groupby('project_id')['response_time'].diff(1).fillna(0)
    # æ»åç‰¹å¾ï¼ˆå‰1æœŸã€å‰2æœŸå€¼ï¼‰
    df['response_time_lag_1'] = df.groupby('project_id')['response_time'].shift(1).fillna(0)
    df['response_time_lag_2'] = df.groupby('project_id')['response_time'].shift(2).fillna(0)

    return df


# ==================== è„šæœ¬æ¨¡å¼ï¼ˆç”¨äºç›´æ¥è¿è¡Œæµ‹è¯•ï¼‰ ====================
if __name__ == "__main__":
    print("æµ‹è¯•å“åº”æ—¶é—´é¢„æµ‹å‡½æ•°...")

    def progress_callback(progress, message):
        print(f"[{progress}%] {message}")

    try:
        result = predict_response_time(progress_callback=progress_callback)
        print("\nâœ… é¢„æµ‹æˆåŠŸï¼")
        print(f"ğŸ“Š æœ‰æ•ˆæ ·æœ¬æ•°: {result['metadata']['valid_samples']}")
        print(f"ğŸ¯ RÂ² æµ‹è¯•é›†: {result['model_evaluation']['XGBoost']['r2_test']}")
        print(f"ğŸ“ˆ RMSE: {result['model_evaluation']['XGBoost']['rmse']}")
        print(f"ğŸ”® æœªæ¥é¢„æµ‹: {result['future_prediction']['prediction_time_points']}")
        print(f"   é¢„æµ‹å€¼: {result['future_prediction']['predicted_response_time']}")

        # ä¿å­˜ç»“æœåˆ° JSON
        import json
        json_path = r'C:\Users\22390\Desktop\OpenSODA\backendData\response_time_prediction_result.json'
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=4)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {json_path}")

    except Exception as e:
        print(f"\nâŒ é¢„æµ‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
